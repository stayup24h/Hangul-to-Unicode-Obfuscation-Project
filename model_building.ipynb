{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stayup24h/Hangul-to-Unicode-Obfuscation-Project/blob/main/model_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pHqopzICGLpl"
   },
   "outputs": [],
   "source": [
    "# initial\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Reshape, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f90a1c79"
   },
   "outputs": [],
   "source": [
    "#모델 설계\n",
    "\n",
    "def model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN layers\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Reshape for RNN layers\n",
    "    new_height = input_shape[0] // 8\n",
    "    new_width = input_shape[1] // 8\n",
    "    model.add(Reshape(target_shape=(new_width, new_height * 128)))\n",
    "\n",
    "    # RNN layers (Bidirectional LSTM)\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa0fa569"
   },
   "outputs": [],
   "source": [
    "# 모델 체크포인트 관리 클래스\n",
    "\n",
    "checkpoint_path = \"training_checkpoints/epoch_{epoch:04d}/model.weights.h5\"\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "class CheckpointManager:\n",
    "    def __init__(self, checkpoint_path_template, max_recent=3, extra_interval=50):\n",
    "        self.checkpoint_path_template = checkpoint_path_template\n",
    "        self.max_recent = max_recent\n",
    "        self.extra_interval = extra_interval\n",
    "        self.recent_epochs = []\n",
    "\n",
    "    def load(self, model, epoch, checkpoint_path_template=checkpoint_path):\n",
    "        path = checkpoint_path_template.format(epoch=epoch)\n",
    "        if os.path.exists(path):\n",
    "            model.load_weights(path)\n",
    "            print(f\"Loaded weights from {path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"No checkpoint found at {path}\")\n",
    "            return False\n",
    "\n",
    "    def save(self, model, epoch):\n",
    "        # Always save at extra_interval epochs\n",
    "        if epoch % self.extra_interval == 0 and epoch not in self.recent_epochs:\n",
    "            path = self.checkpoint_path_template.format(epoch=epoch)\n",
    "            model.save_weights(path)\n",
    "            print(f\"Extra checkpoint saved for epoch {epoch} at {path}\")\n",
    "\n",
    "        # Save recent checkpoints\n",
    "        self.recent_epochs.append(epoch)\n",
    "        if len(self.recent_epochs) > self.max_recent:\n",
    "            # Remove oldest checkpoint from recent\n",
    "            old_epoch = self.recent_epochs.pop(0)\n",
    "            old_path = self.checkpoint_path_template.format(epoch=old_epoch)\n",
    "            if os.path.exists(old_path):\n",
    "                os.remove(old_path)\n",
    "                print(f\"Removed old checkpoint at {old_path}\")\n",
    "\n",
    "        path = self.checkpoint_path_template.format(epoch=epoch)\n",
    "        model.save_weights(path)\n",
    "        print(f\"Checkpoint saved for epoch {epoch} at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0UZbM/LP9PJyk3I/d6VMI",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
