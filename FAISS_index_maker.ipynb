{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcef0b1",
   "metadata": {},
   "source": [
    "# FAISS 인덱스 제작\n",
    "이 노트북은 단계별로 이미지를 생성/불러와 OCR 모델로 벡터를 추출하고 FAISS 인덱스를 만드는 예시입니다.\n",
    "각 코드 셀을 순서대로 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad50947",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pillow tensorflow faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65122e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트 및 스크립트 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('모듈 로드 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline helper 함수들 정의\n",
    "import math\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# faiss는 나중에 설치되어야 합니다. 노트북 상단에서 설치 커맨드를 실행하세요.\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 한글 음절 분해/평가용 슬라이스 (모델 출력 인덱스에 맞춤)\n",
    "L_SLICE = slice(0, 19)\n",
    "V_SLICE = slice(19, 40)\n",
    "T_SLICE = slice(40, 68)\n",
    "\n",
    "# 손실/지표 함수 (모델 로드에 필요할 수 있음)\n",
    "def hangul_loss(y_true, logits):\n",
    "    cho  = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, L_SLICE], logits=logits[:, L_SLICE])\n",
    "    jung = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, V_SLICE], logits=logits[:, V_SLICE])\n",
    "    jong = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, T_SLICE], logits=logits[:, T_SLICE])\n",
    "    return cho + jung + jong\n",
    "\n",
    "\n",
    "def acc_first(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, L_SLICE], -1), tf.argmax(y_pred[:, L_SLICE], -1)), tf.float32))\n",
    "\n",
    "def acc_middle(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, V_SLICE], -1), tf.argmax(y_pred[:, V_SLICE], -1)), tf.float32))\n",
    "\n",
    "def acc_last(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, T_SLICE], -1), tf.argmax(y_pred[:, T_SLICE], -1)), tf.float32))\n",
    "\n",
    "def acc_joint(y_true, y_pred):\n",
    "    l_ok = tf.equal(tf.argmax(y_true[:, L_SLICE], -1), tf.argmax(y_pred[:, L_SLICE], -1))\n",
    "    v_ok = tf.equal(tf.argmax(y_true[:, V_SLICE], -1), tf.argmax(y_pred[:, V_SLICE], -1))\n",
    "    t_ok = tf.equal(tf.argmax(y_true[:, T_SLICE], -1), tf.argmax(y_pred[:, T_SLICE], -1))\n",
    "    return tf.reduce_mean(tf.cast(l_ok & v_ok & t_ok, tf.float32))\n",
    "\n",
    "\n",
    "# 글자 렌더링 함수 (노트북 환경에서 바로 사용)\n",
    "def render_char_to_tensor(char: str, image_size=(128,128), font_path: str = None, font_size: int = 96):\n",
    "    W, H = image_size\n",
    "    try:\n",
    "        if font_path and os.path.exists(font_path):\n",
    "            font = ImageFont.truetype(font_path, font_size)\n",
    "        else:\n",
    "            font = ImageFont.load_default()\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    image = Image.new(\"L\", (W, H), \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    try:\n",
    "        bbox = draw.textbbox((0, 0), char, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        position = ((W - text_width) / 2 - bbox[0], (H - text_height) / 2 - bbox[1])\n",
    "    except Exception:\n",
    "        position = (5,5)\n",
    "\n",
    "    draw.text(position, char, font=font, fill=\"black\")\n",
    "    arr = np.array(image, dtype=np.float32) / 255.0\n",
    "    return arr.reshape((H, W, 1))\n",
    "\n",
    "\n",
    "# 텐서 로드 또는 코드포인트 범위로부터 생성\n",
    "def load_or_generate_tensors(tensors_path, labels_path, start, end, font_path):\n",
    "    if tensors_path and os.path.exists(tensors_path):\n",
    "        print(f\"Loading tensors from {tensors_path}\")\n",
    "        X = np.load(tensors_path)\n",
    "        if labels_path and os.path.exists(labels_path):\n",
    "            labels = np.load(labels_path, allow_pickle=True)\n",
    "        else:\n",
    "            raise ValueError(\"Labels file required when providing tensors. Use --labels <file>.\")\n",
    "        return X, labels\n",
    "\n",
    "    if start is None or end is None:\n",
    "        raise ValueError(\"Either provide tensors file or start/end to generate.\")\n",
    "\n",
    "    labels = []\n",
    "    tensors = []\n",
    "    print(f\"Rendering codepoints from {start} to {end-1} using font={font_path}\")\n",
    "    for cp in range(start, end):\n",
    "        try:\n",
    "            ch = chr(cp)\n",
    "        except Exception:\n",
    "            continue\n",
    "        img = render_char_to_tensor(ch, font_path=font_path)\n",
    "        tensors.append(img)\n",
    "        labels.append(cp)\n",
    "\n",
    "    X = np.stack(tensors, axis=0).astype(np.float32)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    return X, labels\n",
    "\n",
    "\n",
    "# 모델로부터 벡터를 추출하는 함수 (배치 처리)\n",
    "def extract_vectors(model, X: np.ndarray, batch_size: int = 256, encoder=None):\n",
    "    N = X.shape[0]\n",
    "    parts = []\n",
    "    for i in range(0, N, batch_size):\n",
    "        batch = X[i:i+batch_size].astype(np.float32)\n",
    "        preds = model.predict(batch, verbose=0)\n",
    "        if encoder is not None:\n",
    "            preds = encoder.predict(preds, verbose=0)\n",
    "        parts.append(preds.astype(np.float32))\n",
    "    vectors = np.concatenate(parts, axis=0)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "# FAISS 인덱스 빌드 함수\n",
    "def ensure_faiss():\n",
    "    if faiss is None:\n",
    "        raise RuntimeError(\"faiss not available. Install with 'pip install faiss-cpu'.\")\n",
    "\n",
    "\n",
    "def build_index(vectors: np.ndarray, mapping: np.ndarray, out_index_path: str, nprobe: int = 20):\n",
    "    ensure_faiss()\n",
    "    n, d = vectors.shape\n",
    "    print(f\"Building FAISS index for {n} vectors, dim={d}\")\n",
    "    if n > 5000:\n",
    "        nlist = int(max(64, math.sqrt(n)))\n",
    "        quantizer = faiss.IndexFlatL2(d)\n",
    "        index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "        print(f\"Training IVF index with nlist={nlist}...\")\n",
    "        index.train(vectors)\n",
    "        index.add(vectors)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(d)\n",
    "        index.add(vectors)\n",
    "    try:\n",
    "        index.nprobe = nprobe\n",
    "    except Exception:\n",
    "        pass\n",
    "    faiss.write_index(index, out_index_path)\n",
    "    print(f\"Saved FAISS index to {out_index_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e9df1",
   "metadata": {},
   "source": [
    "## 1) 이미지 텐서 생성 또는 불러오기\n",
    "- 기존 `.npy` 텐서가 있으면 해당 파일을 사용하세요.\n",
    "- 없으면 코드포인트 범위를 지정해 폰트로 렌더링하여 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ae4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성/불러기 설정 예시\n",
    "TENSORS_PATH = None  # 예: './unicode_tensors.npy'\n",
    "LABELS_PATH = None   # 예: './unicode_labels.npy'\n",
    "START_CP = 44032     # 한글 시작: 0xAC00\n",
    "END_CP = 55204       # 한글 끝+1: 0xD7A4 -> 55204\n",
    "FONT_PATH = './NotoSansKR-Medium.ttf'  # 폰트 경로 (없으면 기본 폰트 사용)\n",
    "\n",
    "# 텐서 로드 또는 생성 (실행하면 X, labels 변수가 생성됩니다)\n",
    "X, labels = load_or_generate_tensors(TENSORS_PATH, LABELS_PATH, START_CP, END_CP, FONT_PATH)\n",
    "print('이미지 텐서 수:', X.shape, '레이블 수:', labels.shape)\n",
    "# 필요하면 파일로 저장\n",
    "np.save('unicode_tensors.npy', X)\n",
    "np.save('unicode_labels.npy', labels)\n",
    "print('텐서와 레이블 저장 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff62d9e",
   "metadata": {},
   "source": [
    "## 2) OCR 모델 로드\n",
    "- 학습한 `final.keras` 모델 파일 경로를 지정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCR 모델 로드\n",
    "MODEL_PATH = \"./final.keras\"\n",
    "\n",
    "L_SLICE = slice(0, 19)\n",
    "V_SLICE = slice(19, 40)\n",
    "T_SLICE = slice(40, 68)\n",
    "\n",
    "def hangul_loss(y_true, logits):\n",
    "    cho  = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, L_SLICE], logits=logits[:, L_SLICE])\n",
    "    jung = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, V_SLICE], logits=logits[:, V_SLICE])\n",
    "    jong = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, T_SLICE], logits=logits[:, T_SLICE])\n",
    "    return cho + jung + jong\n",
    "\n",
    "def acc_first(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, L_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, L_SLICE], -1)), tf.float32))\n",
    "def acc_middle(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, V_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, V_SLICE], -1)), tf.float32))\n",
    "def acc_last(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, T_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, T_SLICE], -1)), tf.float32))\n",
    "def acc_joint(y_true, y_pred):\n",
    "    l_ok = tf.equal(tf.argmax(y_true[:, L_SLICE], -1), tf.argmax(y_pred[:, L_SLICE], -1))\n",
    "    v_ok = tf.equal(tf.argmax(y_true[:, V_SLICE], -1), tf.argmax(y_pred[:, V_SLICE], -1))\n",
    "    t_ok = tf.equal(tf.argmax(y_true[:, T_SLICE], -1), tf.argmax(y_pred[:, T_SLICE], -1))\n",
    "    return tf.reduce_mean(tf.cast(l_ok & v_ok & t_ok, tf.float32))\n",
    "\n",
    "# --- 모델 로드 ---\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\n",
    "        \"hangul_loss\": hangul_loss,\n",
    "        \"acc_first\": acc_first,\n",
    "        \"acc_middle\": acc_middle,\n",
    "        \"acc_last\": acc_last,\n",
    "        \"acc_joint\": acc_joint,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed43481",
   "metadata": {},
   "source": [
    "## 3) 벡터 추출\n",
    "- OCR 모델 출력(예: 68차원)을 그대로 사용하거나, 필요하면 인코더를 통해 차원 축소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484730b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 추출 예시\n",
    "BATCH_SIZE = 256\n",
    "encoder_model_path = None  # 예: './best_simple_encoder_for_faiss.h5'\n",
    "encoder = None\n",
    "if encoder_model_path:\n",
    "    encoder = tf.keras.models.load_model(encoder_model_path)\n",
    "\n",
    "vectors = extract_vectors(model, X, batch_size=BATCH_SIZE, encoder=encoder)\n",
    "print('벡터 형상:', vectors.shape)\n",
    "np.save('unicode_vectors.npy', vectors)\n",
    "print('벡터 저장: unicode_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e9591",
   "metadata": {},
   "source": [
    "## 4) FAISS 인덱스 생성 및 저장\n",
    "- 데이터 수에 따라 Flat 또는 IVF 인덱스를 자동으로 선택합니다.\n",
    "- `nprobe`는 검색시 탐색할 클러스터 개수로, 정확도/속도 조정을 위해 변경 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스 빌드\n",
    "OUT_INDEX = 'unicode_faiss_index.faiss'\n",
    "OUT_MAP = 'faiss_unicode_map.npy'\n",
    "NPROBE = 20\n",
    "\n",
    "# build_index 함수 사용 (노트북 내부 정의)\n",
    "build_index(vectors, labels, OUT_INDEX, nprobe=NPROBE)\n",
    "np.save(OUT_MAP, labels)\n",
    "print('인덱스 및 매핑 저장 완료:', OUT_INDEX, OUT_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d71be",
   "metadata": {},
   "source": [
    "## 5) 간단 검색 예시\n",
    "- 생성한 FAISS 인덱스와 매핑을 불러와 샘플 문자로 검색해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64145a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.read_index(OUT_INDEX)\n",
    "u_map = np.load(OUT_MAP)\n",
    "\n",
    "# 예시 입력 문자\n",
    "query_char = '일'\n",
    "q_tensor = render_char_to_tensor(query_char, font_path=FONT_PATH)\n",
    "q_tensor = np.expand_dims(q_tensor, 0).astype(np.float32)\n",
    "q_vec = model.predict(q_tensor)\n",
    "if encoder is not None:\n",
    "    q_vec = encoder.predict(q_vec)\n",
    "\n",
    "D, I = index.search(q_vec, 10)\n",
    "print('Top results:')\n",
    "for rank, (idx, dist) in enumerate(zip(I[0], D[0])):\n",
    "    if idx >= 0:\n",
    "        print(f\"{rank+1}. codepoint={u_map[idx]} char={chr(int(u_map[idx]))} dist={dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
