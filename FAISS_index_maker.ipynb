{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcef0b1",
   "metadata": {},
   "source": [
    "# FAISS 인덱스 제작\n",
    "이 노트북은 단계별로 이미지를 생성/불러와 OCR 모델로 벡터를 추출하고 FAISS 인덱스를 만드는 예시입니다.\n",
    "각 코드 셀을 순서대로 실행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad50947",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pillow tensorflow faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65122e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트 및 스크립트 불러오기\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('모듈 로드 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e9df1",
   "metadata": {},
   "source": [
    "## 1) 이미지 텐서 생성 또는 불러오기\n",
    "- 기존 `.npy` 텐서가 있으면 해당 파일을 사용하세요.\n",
    "- 없으면 코드포인트 범위를 지정해 폰트로 렌더링하여 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ae4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 생성/불러기 설정 예시\n",
    "TENSORS_PATH = None  # 예: './unicode_tensors.npy'\n",
    "LABELS_PATH = None   # 예: './unicode_labels.npy'\n",
    "START_CP = 44032     # 한글 시작: 0xAC00\n",
    "END_CP = 55204       # 한글 끝+1: 0xD7A4 -> 55204\n",
    "FONT_PATH = './NotoSansKR-Medium.ttf'  # 폰트 경로 (없으면 기본 폰트 사용)\n",
    "\n",
    "# 텐서 로드 또는 생성 (실행하면 X, labels 변수가 생성됩니다)\n",
    "X, labels = bf.load_or_generate_tensors(TENSORS_PATH, LABELS_PATH, START_CP, END_CP, FONT_PATH)\n",
    "print('이미지 텐서 수:', X.shape, '레이블 수:', labels.shape)\n",
    "# 필요하면 파일로 저장\n",
    "np.save('unicode_tensors.npy', X)\n",
    "np.save('unicode_labels.npy', labels)\n",
    "print('텐서와 레이블 저장 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff62d9e",
   "metadata": {},
   "source": [
    "## 2) OCR 모델 로드\n",
    "- 학습한 `final.keras` 모델 파일 경로를 지정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCR 모델 로드\n",
    "MODEL_PATH = \"./final.keras\"\n",
    "\n",
    "L_SLICE = slice(0, 19)\n",
    "V_SLICE = slice(19, 40)\n",
    "T_SLICE = slice(40, 68)\n",
    "\n",
    "def hangul_loss(y_true, logits):\n",
    "    cho  = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, L_SLICE], logits=logits[:, L_SLICE])\n",
    "    jung = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, V_SLICE], logits=logits[:, V_SLICE])\n",
    "    jong = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, T_SLICE], logits=logits[:, T_SLICE])\n",
    "    return cho + jung + jong\n",
    "\n",
    "def acc_first(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, L_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, L_SLICE], -1)), tf.float32))\n",
    "def acc_middle(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, V_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, V_SLICE], -1)), tf.float32))\n",
    "def acc_last(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, T_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, T_SLICE], -1)), tf.float32))\n",
    "def acc_joint(y_true, y_pred):\n",
    "    l_ok = tf.equal(tf.argmax(y_true[:, L_SLICE], -1), tf.argmax(y_pred[:, L_SLICE], -1))\n",
    "    v_ok = tf.equal(tf.argmax(y_true[:, V_SLICE], -1), tf.argmax(y_pred[:, V_SLICE], -1))\n",
    "    t_ok = tf.equal(tf.argmax(y_true[:, T_SLICE], -1), tf.argmax(y_pred[:, T_SLICE], -1))\n",
    "    return tf.reduce_mean(tf.cast(l_ok & v_ok & t_ok, tf.float32))\n",
    "\n",
    "# --- 모델 로드 ---\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\n",
    "        \"hangul_loss\": hangul_loss,\n",
    "        \"acc_first\": acc_first,\n",
    "        \"acc_middle\": acc_middle,\n",
    "        \"acc_last\": acc_last,\n",
    "        \"acc_joint\": acc_joint,\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed43481",
   "metadata": {},
   "source": [
    "## 3) 벡터 추출\n",
    "- OCR 모델 출력(예: 68차원)을 그대로 사용하거나, 필요하면 인코더를 통해 차원 축소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484730b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 추출 예시\n",
    "BATCH_SIZE = 256\n",
    "encoder_model_path = None  # 예: './best_simple_encoder_for_faiss.h5'\n",
    "encoder = None\n",
    "if encoder_model_path:\n",
    "    encoder = tf.keras.models.load_model(encoder_model_path)\n",
    "\n",
    "vectors = bf.extract_vectors(model, X, batch_size=BATCH_SIZE, encoder=encoder)\n",
    "print('벡터 형상:', vectors.shape)\n",
    "np.save('unicode_vectors.npy', vectors)\n",
    "print('벡터 저장: unicode_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e9591",
   "metadata": {},
   "source": [
    "## 4) FAISS 인덱스 생성 및 저장\n",
    "- 데이터 수에 따라 Flat 또는 IVF 인덱스를 자동으로 선택합니다.\n",
    "- `nprobe`는 검색시 탐색할 클러스터 개수로, 정확도/속도 조정을 위해 변경 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스 빌드\n",
    "OUT_INDEX = 'unicode_faiss_index.faiss'\n",
    "OUT_MAP = 'faiss_unicode_map.npy'\n",
    "NPROBE = 20\n",
    "\n",
    "bf.build_index(vectors, labels, OUT_INDEX, nprobe=NPROBE)\n",
    "np.save(OUT_MAP, labels)\n",
    "print('인덱스 및 매핑 저장 완료:', OUT_INDEX, OUT_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d71be",
   "metadata": {},
   "source": [
    "## 5) 간단 검색 예시\n",
    "- 생성한 FAISS 인덱스와 매핑을 불러와 샘플 문자로 검색해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64145a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.read_index(OUT_INDEX)\n",
    "u_map = np.load(OUT_MAP)\n",
    "\n",
    "# 예시 입력 문자\n",
    "query_char = '일'\n",
    "q_tensor = bf.render_char_to_tensor(query_char, font_path=FONT_PATH)\n",
    "q_tensor = np.expand_dims(q_tensor, 0).astype(np.float32)\n",
    "q_vec = model.predict(q_tensor)\n",
    "if encoder is not None:\n",
    "    q_vec = encoder.predict(q_vec)\n",
    "\n",
    "D, I = index.search(q_vec, 10)\n",
    "print('Top results:')\n",
    "for rank, (idx, dist) in enumerate(zip(I[0], D[0])):\n",
    "    if idx >= 0:\n",
    "        print(f\"{rank+1}. codepoint={u_map[idx]} char={chr(int(u_map[idx]))} dist={dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
