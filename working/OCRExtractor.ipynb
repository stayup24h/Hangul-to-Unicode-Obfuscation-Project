{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b380dd",
   "metadata": {},
   "source": [
    "## OCR ê²°ê³¼ ì¶”ì¶œê¸°\n",
    "í•™ìŠµí•œ ìœ ë‹ˆì½”ë“œ ê²°ê³¼ë¥¼ ë¡œì»¬ì—ì„œ OCR ëª¨ë¸ì— í†µê³¼ì‹œì¼œ íŒŒì¼ë¡œ ì¶œë ¥, ì´í›„ ì‚¬ìš©  \n",
    "ChatGPT ì‚¬ìš©í•˜ì—¬ ì£¼ì„ ë° ì˜¤ë¥˜ ìˆ˜ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be77fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded.\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_0.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_0.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_0.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_1.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_1.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_1.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_10.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_10.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_10.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_11.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_11.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 76ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_11.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_12.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_12.npy (shape=(631, 128, 128, 1))\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step \n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_12.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_2.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_2.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_2.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_3.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_3.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_3.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_4.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_4.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_4.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_5.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_5.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_5.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_6.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_6.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_6.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_7.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_7.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_7.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_8.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_8.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_8.npy\n",
      "\n",
      "â–¶ Predicting /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_tensors_9.npy -> /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_9.npy (shape=(16384, 128, 128, 1))\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 75ms/step\n",
      "âœ… Saved /home/voxfish_ljy$/virtual/Unicodes/combined_unicode_results_9.npy\n",
      "\n",
      "ğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# === ì„¤ì • ===\n",
    "MODEL_PATH = \"./runs_MobileNetV2/final.keras\"\n",
    "INPUT_DIR  = \"/home/voxfish_ljy$/virtual/Unicodes\"     # unicode_tensors_N.npyê°€ ë“¤ì–´ ìˆëŠ” í´ë”\n",
    "OUTPUT_DIR = \"/home/voxfish_ljy$/virtual/Unicodes\"    # unicode_results_N.npyë¥¼ ì €ì¥í•  í´ë”\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- í•„ìš”í•œ custom_objects ì •ì˜ ---\n",
    "L_SLICE = slice(0, 19)\n",
    "V_SLICE = slice(19, 40)\n",
    "T_SLICE = slice(40, 68)\n",
    "\n",
    "def hangul_loss(y_true, logits):\n",
    "    cho  = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, L_SLICE], logits=logits[:, L_SLICE])\n",
    "    jung = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, V_SLICE], logits=logits[:, V_SLICE])\n",
    "    jong = tf.nn.softmax_cross_entropy_with_logits(labels=y_true[:, T_SLICE], logits=logits[:, T_SLICE])\n",
    "    return cho + jung + jong\n",
    "\n",
    "def acc_first(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, L_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, L_SLICE], -1)), tf.float32))\n",
    "def acc_middle(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, V_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, V_SLICE], -1)), tf.float32))\n",
    "def acc_last(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true[:, T_SLICE], -1),\n",
    "                                           tf.argmax(y_pred[:, T_SLICE], -1)), tf.float32))\n",
    "def acc_joint(y_true, y_pred):\n",
    "    l_ok = tf.equal(tf.argmax(y_true[:, L_SLICE], -1), tf.argmax(y_pred[:, L_SLICE], -1))\n",
    "    v_ok = tf.equal(tf.argmax(y_true[:, V_SLICE], -1), tf.argmax(y_pred[:, V_SLICE], -1))\n",
    "    t_ok = tf.equal(tf.argmax(y_true[:, T_SLICE], -1), tf.argmax(y_pred[:, T_SLICE], -1))\n",
    "    return tf.reduce_mean(tf.cast(l_ok & v_ok & t_ok, tf.float32))\n",
    "\n",
    "# --- ëª¨ë¸ ë¡œë“œ ---\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\n",
    "        \"hangul_loss\": hangul_loss,\n",
    "        \"acc_first\": acc_first,\n",
    "        \"acc_middle\": acc_middle,\n",
    "        \"acc_last\": acc_last,\n",
    "        \"acc_joint\": acc_joint,\n",
    "    }\n",
    ")\n",
    "print(\"âœ… Model loaded.\")\n",
    "\n",
    "# --- ì…ë ¥ íŒŒì¼ ëª©ë¡ ---\n",
    "input_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"combined_unicode_tensors_*.npy\")))\n",
    "if not input_files:\n",
    "    raise FileNotFoundError(\"ì…ë ¥ í´ë”ì— combined_unicode_tensors_*.npy íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "for f_in in input_files:\n",
    "    base = os.path.basename(f_in)                   # unicode_tensors_N.npy\n",
    "    f_out = base.replace(\"combined_unicode_tensors_\", \"combined_unicode_results_\")\n",
    "    out_path = os.path.join(OUTPUT_DIR, f_out)\n",
    "\n",
    "    # ì…ë ¥ ë°ì´í„° ë¡œë“œ\n",
    "    X = np.load(f_in)                               # shape: (N, 128,128,1)\n",
    "    if X.ndim == 3:\n",
    "        X = np.expand_dims(X, -1)\n",
    "    X = X.astype(np.float32)\n",
    "    if np.max(X) > 1.5:\n",
    "        X /= 255.0                                  # ì •ê·œí™”\n",
    "\n",
    "    print(f\"â–¶ Predicting {f_in} -> {out_path} (shape={X.shape})\")\n",
    "\n",
    "    # ëª¨ë¸ ì˜ˆì¸¡\n",
    "    preds = model.predict(X, batch_size=256, verbose=1)  # (N,68)\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    np.save(out_path, preds.astype(np.float32))\n",
    "    print(f\"âœ… Saved {out_path}\\n\")\n",
    "\n",
    "print(\"ğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ffb06",
   "metadata": {},
   "source": [
    "## OCR í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥\n",
    "ëœë¤í•œ ìœ ë‹ˆì½”ë“œê°€ ì˜ ì¶œë ¥ë˜ê³ , OCRì´ ê¸°ëŠ¥í•˜ëŠ”ì§€ í™•ì¸  \n",
    "ì „ì²´ ì½”ë“œ GPTë¥¼ ì´ìš©í•˜ì—¬ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1414c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Shard 10, Index 920]\n",
      "ì…ë ¥ ì´ë¯¸ì§€:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAADFklEQVR4nO2aTUhUURTH75P5qCmxRWQgRh8UQYVZQ1FhYJ9G0qIiihZRtAnbtYq2EW6CGWx2tgtBkBYSGUpQWVFKaAoNtYmmD0HDEPNjZtTTorR37r1j9755Z4bgnN3/3vPO+b07b+47997ngCiulRQ5PwMwAAMwAAMwAAMwAAMwAAMwgAcAaDvguC3aOr+4Wu1qfWQez9K6dyohtvcDAMCwu63DNJ4lQPaGbshCcQCArgIATB7PMY5XZwFuewIIGP9WQojpumc5eu7MJcSATai/ZnH/sycXidMI1W5J8hPcXOxGAo+XUAO8CVoMrDGAxTzQkLUAMDZzgLZXFPktAG6R5DcHeNEnt4QvtqfS6ZGepsNOPgSmD8sl+cLaTwt9A3uVsL7/C2ZWShlOZ129mbPkAE+lBFunUHem2iuA6TPwRNIJNO2IYNwwjmKmAC+x3Ldf6q/ZRQzQi+U1xaGBFmBkFMnwEcXjKC3Aeyxrlike5dtIAT5iKT8BQgixhxRgGMtNGpcNhQTYWHCAH1iu07isIQWYxrJU46I+l34CpJEK60rZCCkALoa0uWhHAN+y9iKbitEeIITUpLdc+QCEkZqa07hMkAKU/TvZGCnAKiw/Fxvgg8ZliBRAmuZ0AO9IAaS3z2uNS5IUoBLPPV0Z1eU5KYCDq95xuUgWoi9FCiCXGzHFodVbfmMAqQZ62CP1f08QAxxcivXln1hfl7TvABGp6h08j16Qjc0e85svTu/LF+5ILvR9O6eE9X+LZqZSzhGsbxkcTX/tv3tKUwoQ7BHFrAaWYI/oiq4Szd/MAUKxIgOIE8omidvWHyMHEE3R3H2BFtqFiRBCiMiDnEmc5t26IsVnAFHenWMJXJK4IL54A7A8Lxg7o4tR1g4AFaiJ6sAC4F6Fkv9QCgBgRYEAYCK+GY1+befv9rA3AMfL53xvO3uTQ+OwvHTtlmj9n3pVmtI66gxjWZ2YzFtVldqmKdKMzLdzQ68fRv6HB5cMwAAMwAAMwAAMwAAM4LN5Ksv9tKKPAAMwAAMwAAMwAAMwAAMwAAMwwC/4+6+uF+AljAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì˜ˆì¸¡ëœ ì´ˆì„±/ì¤‘ì„±/ì¢…ì„± (argmax ê¸°ì¤€): ã…‡ã…£\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ===== ê²½ë¡œ ì„¤ì •(í•„ìš”ì— ë§ê²Œ ìˆ˜ì •) =====\n",
    "TENSOR_DIR = r\"/home/voxfish_ljy$/virtual/Unicodes\"   # unicode_tensors_*.npy ìˆëŠ” í´ë”\n",
    "RESULT_DIR = r\"/home/voxfish_ljy$/virtual/Unicodes\"   # unicode_results_*.npy ìˆëŠ” í´ë”\n",
    "\n",
    "# 68ì°¨ì› ë²¡í„°ë¥¼ ì´ˆ/ì¤‘/ì¢…ì„±ìœ¼ë¡œ í•´ì„í•˜ê³  ì‹¶ì„ ë•Œë§Œ ì‚¬ìš© (ì„ íƒ)\n",
    "L_SLICE = slice(0, 19)\n",
    "V_SLICE = slice(19, 40)\n",
    "T_SLICE = slice(40, 68)\n",
    "\n",
    "CHOS = [\"ã„±\",\"ã„²\",\"ã„´\",\"ã„·\",\"ã„¸\",\"ã„¹\",\"ã…\",\"ã…‚\",\"ã…ƒ\",\"ã……\",\"ã…†\",\"ã…‡\",\"ã…ˆ\",\"ã…‰\",\"ã…Š\",\"ã…‹\",\"ã…Œ\",\"ã…\",\"ã…\"]\n",
    "JUNS = [\"ã…\",\"ã…\",\"ã…‘\",\"ã…’\",\"ã…“\",\"ã…”\",\"ã…•\",\"ã…–\",\"ã…—\",\"ã…˜\",\"ã…™\",\"ã…š\",\"ã…›\",\"ã…œ\",\"ã…\",\"ã…\",\"ã…Ÿ\",\"ã… \",\"ã…¡\",\"ã…¢\",\"ã…£\"]\n",
    "JONGS = [\"\",\"ã„±\",\"ã„²\",\"ã„³\",\"ã„´\",\"ã„µ\",\"ã„¶\",\"ã„·\",\"ã„¹\",\"ã„º\",\"ã„»\",\"ã„¼\",\"ã„½\",\"ã„¾\",\"ã„¿\",\"ã…€\",\"ã…\",\"ã…‚\",\"ã…„\",\"ã……\",\"ã…†\",\"ã…‡\",\"ã…ˆ\",\"ã…Š\",\"ã…‹\",\"ã…Œ\",\"ã…\",\"ã…\"]\n",
    "\n",
    "def decode_LVT(vec):\n",
    "    \"\"\"68ì°¨ì› ë²¡í„°ë¥¼ (ì´ˆì„±, ì¤‘ì„±, ì¢…ì„±) ë¬¸ìë¡œ ëŒ€ëµ í•´ì„ (ì„ íƒ ê¸°ëŠ¥)\"\"\"\n",
    "    l_idx = np.argmax(vec[L_SLICE])\n",
    "    v_idx = np.argmax(vec[V_SLICE])\n",
    "    t_idx = np.argmax(vec[T_SLICE])\n",
    "    return CHOS[l_idx], JUNS[v_idx], JONGS[t_idx]\n",
    "\n",
    "\n",
    "def show_png_from_tensor(x):\n",
    "    \"\"\"\n",
    "    (H,W) ë˜ëŠ” (H,W,1) í…ì„œë¥¼ Jupyterì— ì´ë¯¸ì§€ë¡œ í‘œì‹œ\n",
    "    x: numpy array, ê°’ ë²”ìœ„ê°€ 0/1 ë˜ëŠ” 0~255ë¼ê³  ê°€ì •\n",
    "    \"\"\"\n",
    "    arr = x\n",
    "    if arr.ndim == 2:\n",
    "        arr = arr[..., None]      # (H,W,1)ë¡œ\n",
    "    # ê°’ ìŠ¤ì¼€ì¼ ë§ì¶°ì„œ uint8ë¡œ\n",
    "    if arr.max() <= 1.0:\n",
    "        arr = (arr * 255).astype(np.uint8)\n",
    "    else:\n",
    "        arr = arr.astype(np.uint8)\n",
    "    png_bytes = tf.io.encode_png(tf.constant(arr)).numpy()\n",
    "    display(Image(data=png_bytes))\n",
    "\n",
    "\n",
    "def check_random_pair(tensor_dir=TENSOR_DIR, result_dir=RESULT_DIR):\n",
    "    # 1) íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘\n",
    "    tensor_files = sorted(glob.glob(os.path.join(tensor_dir, \"unicode_tensors_*.npy\"))) or sorted(glob.glob(os.path.join(tensor_dir, \"combined_unicode_tensors_*.npy\")))\n",
    "    result_files = sorted(glob.glob(os.path.join(result_dir, \"unicode_results_*.npy\"))) or sorted(glob.glob(os.path.join(tensor_dir, \"combined_unicode_results_*.npy\")))\n",
    "\n",
    "    assert len(tensor_files) > 0, \"unicode_tensors_*.npy íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    assert len(tensor_files) == len(result_files), \"tensorsì™€ results ìƒ¤ë“œ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
    "\n",
    "    # 2) ëœë¤ ìƒ¤ë“œ ì„ íƒ\n",
    "    shard_idx = random.randint(0, len(tensor_files) - 1)\n",
    "    X = np.load(tensor_files[shard_idx], mmap_mode=\"r\")   # (N,128,128,1) ë˜ëŠ” (N,128,128)\n",
    "    R = np.load(result_files[shard_idx], mmap_mode=\"r\")   # (N,68)\n",
    "\n",
    "    assert X.shape[0] == R.shape[0], f\"ìƒ¤ë“œ {shard_idx}ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: X={X.shape[0]}, R={R.shape[0]}\"\n",
    "\n",
    "    # 3) ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ\n",
    "    i = random.randint(0, X.shape[0] - 1)\n",
    "    x = X[i]\n",
    "    vec = R[i]\n",
    "\n",
    "    # 4) ì…ë ¥ ì´ë¯¸ì§€ í‘œì‹œ\n",
    "    print(f\"[Shard {shard_idx}, Index {i}]\")\n",
    "    print(\"ì…ë ¥ ì´ë¯¸ì§€:\")\n",
    "    show_png_from_tensor(x)\n",
    "\n",
    "    # ì„ íƒ: ì´ˆ/ì¤‘/ì¢…ì„±ìœ¼ë¡œ í•´ì„í•´ì„œ ë³´ê¸° (vecì´ ê·¸ëŸ° êµ¬ì¡°ë¼ë©´)\n",
    "    try:\n",
    "        l, v, t = decode_LVT(vec)\n",
    "        print(f\"\\nì˜ˆì¸¡ëœ ì´ˆì„±/ì¤‘ì„±/ì¢…ì„± (argmax ê¸°ì¤€): {l}{v}{t}\")\n",
    "    except Exception as e:\n",
    "        print(\"\\ndecode_LVTì—ì„œ ì˜¤ë¥˜ ë°œìƒ (ë²¡í„° êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ):\", e)\n",
    "\n",
    "    return x, vec\n",
    "\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ (ëª‡ ë²ˆ ëŒë ¤ë³´ë©´ì„œ í™•ì¸)\n",
    "_ = check_random_pair()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
