{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stayup24h/Hangul-to-Unicode-Obfuscation-Project/blob/main/unicode_similarity_search_by_OCR_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FeIYCo_PWuXB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.model_selection import KFold\n",
        "from copy import deepcopy # 각 폴드마다 새로운 모델을 시작하기 위함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S1plIz5QWj1a"
      },
      "outputs": [],
      "source": [
        "# autoencoder 정의\n",
        "\n",
        "def compiled_autoencoder(input_dim=68, latent_dim=16):\n",
        "    \"\"\"Autoencoder 모델을 정의하고 컴파일하여 반환합니다.\"\"\"\n",
        "\n",
        "    # 1. Encoder 정의\n",
        "    input_layer = tf.keras.layers.Input(shape=(input_dim,), name='input_vector')\n",
        "    x = tf.keras.layers.Dense(32, activation='relu')(input_layer)\n",
        "    latent_vector = tf.keras.layers.Dense(latent_dim, activation='relu', name='latent_vector')(x)\n",
        "    encoder = Model(input_layer, latent_vector, name='encoder')\n",
        "\n",
        "    # 2. Decoder 정의\n",
        "    latent_input = tf.keras.layers.Input(shape=(latent_dim,), name='latent_input')\n",
        "    y = tf.keras.layers.Dense(32, activation='relu')(latent_input)\n",
        "    reconstruction = tf.keras.layers.Dense(input_dim, activation='sigmoid', name='reconstruction')(y)\n",
        "    decoder = Model(latent_input, reconstruction, name='decoder')\n",
        "\n",
        "    # 3. Autoencoder 정의\n",
        "    autoencoder_output = decoder(encoder(input_layer))\n",
        "    autoencoder = Model(input_layer, autoencoder_output, name='autoencoder')\n",
        "\n",
        "    # 4. 컴파일\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder, encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLGfyvR_c3J1"
      },
      "outputs": [],
      "source": [
        "# autoencoder 학습\n",
        "\n",
        "# TODO : 유니코드를 OCR한 68차원 numpy 배열 불러오기\n",
        "\n",
        "N = 200000 # numpy 배열 크기\n",
        "INPUT_DIM = 68\n",
        "X_features_all = np.random.rand(N, INPUT_DIM).astype('float32')\n",
        "LATENT_DIM = 16\n",
        "\n",
        "K_FOLDS = 10\n",
        "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "fold_results = []\n",
        "trained_encoders = []\n",
        "best_loss = float('inf')\n",
        "best_encoder = None\n",
        "\n",
        "print(f\"Starting {K_FOLDS}-Fold Cross-Validation\")\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_features_all)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
        "\n",
        "    X_train, X_val = X_features_all[train_index], X_features_all[val_index]\n",
        "\n",
        "    autoencoder, current_encoder = compiled_autoencoder(INPUT_DIM, LATENT_DIM)\n",
        "\n",
        "    history = autoencoder.fit(\n",
        "        x=X_train,\n",
        "        y=X_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val, X_val),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "    fold_results.append(val_loss)\n",
        "    print(f\"Fold {fold+1} Validation Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_encoder = deepcopy(current_encoder)\n",
        "\n",
        "mean_val_loss = np.mean(fold_results)\n",
        "std_val_loss = np.std(fold_results)\n",
        "\n",
        "print(\"\\n--- K-Fold Summary ---\")\n",
        "print(f\"Individual Fold Losses: {fold_results}\")\n",
        "print(f\"Mean Validation Loss across {K_FOLDS} Folds: {mean_val_loss:.6f} (+/- {std_val_loss:.6f})\")\n",
        "\n",
        "if best_encoder:\n",
        "    best_encoder.save('best_kf_encoder_for_faiss.h5')\n",
        "    print(\"\\nBest Encoder Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G02TOIlKeV9Z"
      },
      "outputs": [],
      "source": [
        "# 68차원 벡터들을 16차원 벡터들로 변환\n",
        "\n",
        "best_encoder = load_model('best_kf_encoder_for_faiss.h5')\n",
        "\n",
        "# TODO : 유니코드를 OCR한 68차원 numpy 배열 불러오기\n",
        "\n",
        "# X_features_all = np.load('all_unicode_features.npy')\n",
        "N = 200000 #numpy 배열 크기\n",
        "INPUT_DIM = 68\n",
        "X_features_all = np.random.rand(N, INPUT_DIM).astype('float32') # 예시 데이터\n",
        "\n",
        "X_16dim_vectors = best_encoder.predict(X_features_all)\n",
        "\n",
        "X_16dim_vectors = X_16dim_vectors.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWRgt5XDe0nj"
      },
      "outputs": [],
      "source": [
        "# 변환한 벡터들 저장\n",
        "\n",
        "VECTOR_OUTPUT_FILENAME = \"faiss_16dim_vectors.npy\"\n",
        "\n",
        "np.save(VECTOR_OUTPUT_FILENAME, X_16dim_vectors)\n",
        "\n",
        "print(f\"16차원 잠재 벡터가 {VECTOR_OUTPUT_FILENAME}에 저장되었습니다. (Shape: {X_16dim_vectors.shape})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIbA6EVsfGgD"
      },
      "outputs": [],
      "source": [
        "# 유니코드 코드 포인트 배열 생성 (N개의 요소)\n",
        "# 이 배열은 X_features_all을 생성했을 때의 순서와 정확히 일치해야 합니다.\n",
        "# 예시: 한글 외 유니코드 코드 포인트만 순서대로 나열\n",
        "# [0x0000, 0x0001, ..., 0x00A0, ..., 0x10FFFF] (제외 범위 제외)\n",
        "unicode_codepoints = np.array([\n",
        "    # 실제 데이터셋을 만들 때 사용된 코드 포인트를 순서대로 여기에 배치해야 함.\n",
        "    i for i in range(N) # 예시에서는 N개의 임의 인덱스라고 가정\n",
        "])\n",
        "\n",
        "MAPPING_OUTPUT_FILENAME = \"faiss_unicode_map.npy\"\n",
        "np.save(MAPPING_OUTPUT_FILENAME, unicode_codepoints)\n",
        "\n",
        "print(f\"유니코드 코드 포인트 매핑이 {MAPPING_OUTPUT_FILENAME}에 저장되었습니다. (Shape: {unicode_codepoints.shape})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install faiss-cpu numpy\n",
        "%pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# --- 저장된 파일 이름 ---\n",
        "VECTOR_OUTPUT_FILENAME = \"faiss_16dim_vectors.npy\"\n",
        "MAPPING_OUTPUT_FILENAME = \"faiss_unicode_map.npy\"\n",
        "QUERY_ENCODER_MODEL = \"best_kf_encoder_for_faiss.h5\"\n",
        "FAISS_INDEX_FILENAME = \"unicode_faiss_index.faiss\"\n",
        "\n",
        "# 1. 16차원 잠재 벡터 로드 (N x 16 형태)\n",
        "# N은 약 20만\n",
        "X_vectors = np.load(VECTOR_OUTPUT_FILENAME)\n",
        "D = X_vectors.shape[1]  # D = 16 (벡터 차원)\n",
        "\n",
        "# 2. 유니코드 매핑 정보 로드 (N개 코드 포인트)\n",
        "# 이 배열의 순서가 X_vectors의 행 순서와 일치해야 합니다.\n",
        "U_map = np.load(MAPPING_OUTPUT_FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- FAISS 인덱스 설정 ---\n",
        "NLIST = 500  # 클러스터(Partition) 개수. (데이터 개수의 sqrt 근처 값 권장)\n",
        "\n",
        "# 1. Quantizer 정의: 벡터를 클러스터링할 때 사용할 기준 (일반적으로 L2 거리를 사용하는 Flat 인덱스 사용)\n",
        "quantizer = faiss.IndexFlatL2(D)\n",
        "\n",
        "# 2. IVF 인덱스 생성: D차원, NLIST개의 클러스터, L2 거리 사용\n",
        "index = faiss.IndexIVFFlat(quantizer, D, NLIST, faiss.METRIC_L2)\n",
        "\n",
        "# 3. 인덱스 훈련 (Training)\n",
        "# IVF 인덱스는 add 전에 클러스터 중심점을 계산하는 훈련이 필요합니다.\n",
        "print(\"FAISS Index Training Start...\")\n",
        "index.train(X_vectors)\n",
        "print(\"Training Complete. Index is trained:\", index.is_trained)\n",
        "\n",
        "# 4. 벡터 추가 (Adding)\n",
        "# 16차원 잠재 벡터를 인덱스에 추가합니다.\n",
        "index.add(X_vectors)\n",
        "print(f\"Total vectors added: {index.ntotal}\")\n",
        "\n",
        "# 5. 검색 속도 vs. 정확도 설정 (nprobe)\n",
        "# 검색 시 확인할 클러스터의 개수. 높을수록 정확하나 느립니다.\n",
        "# 20만 개에서는 10~30 사이가 적절하며, 20을 추천합니다.\n",
        "index.nprobe = 20\n",
        "\n",
        "# 6. 인덱스 저장\n",
        "faiss.write_index(index, FAISS_INDEX_FILENAME)\n",
        "print(f\"FAISS Index saved as {FAISS_INDEX_FILENAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "\n",
        "# Keras 모델 로드 (query_encoder)\n",
        "try:\n",
        "    # Autoencoder 학습에서 저장한 인코더 모델 로드\n",
        "    query_encoder = load_model(QUERY_ENCODER_MODEL) \n",
        "except Exception as e:\n",
        "    # 모델 로드 실패 시 임시 더미 함수 사용 (실제 환경에서는 반드시 로드 필요)\n",
        "    print(f\"경고: 인코더 모델 로드 실패. 실제 사용 시 {QUERY_ENCODER_MODEL}을 로드해야 합니다.\")\n",
        "    query_encoder = lambda x: np.random.rand(1, 16).astype('float32') \n",
        "\n",
        "# FAISS 인덱스 로드\n",
        "index = faiss.read_index(FAISS_INDEX_FILENAME)\n",
        "\n",
        "def find_similar_unicode(korean_char, k=5):\n",
        "    \"\"\"\n",
        "    한글 문자를 입력받아 시각적으로 가장 유사한 K개의 유니코드 문자를 반환합니다.\n",
        "    \"\"\"\n",
        "    \n",
        "    # --- 1. 쿼리 벡터 생성 (OCR 예측 및 인코더 변환 필요) ---\n",
        "    \n",
        "    # 실제 구현 필요: \n",
        "    # 1. korean_char를 이미지로 렌더링\n",
        "    # 2. OCR 모델로 예측하여 68차원 특징 벡터 (cho_vec, jung_vec, jong_vec) 생성\n",
        "    # 3. 68차원 특징 벡터를 하나의 배열로 조합 (q_68dim)\n",
        "    \n",
        "    # 여기서는 68차원 특징 벡터가 이미 준비되었다고 가정하고 변환만 수행\n",
        "    \n",
        "    # 예시: 임의의 68차원 벡터 생성 (실제는 OCR 모델 출력)\n",
        "    dummy_q_68dim = np.random.rand(1, 68).astype('float32') \n",
        "    \n",
        "    # 쿼리 벡터를 16차원 잠재 벡터로 변환\n",
        "    q_16dim = query_encoder.predict(dummy_q_68dim, verbose=0)\n",
        "    q_16dim = q_16dim.astype('float32') # FAISS 요구 타입\n",
        "\n",
        "    # --- 2. FAISS 검색 ---\n",
        "    # D: 거리(Distance) 배열, I: 인덱스(Index) 배열\n",
        "    # 검색된 인덱스 I는 U_map의 인덱스와 일치합니다.\n",
        "    D, I = index.search(q_16dim, k) \n",
        "\n",
        "    # --- 3. 결과 해석 및 반환 ---\n",
        "    results = []\n",
        "    \n",
        "    # 가장 가까운 이웃의 인덱스와 거리를 순회합니다.\n",
        "    for rank, (idx, distance) in enumerate(zip(I[0], D[0])):\n",
        "        if idx >= 0: # 유효한 인덱스일 경우\n",
        "            code_point = U_map[idx]\n",
        "            unicode_char = chr(code_point)\n",
        "            \n",
        "            results.append({\n",
        "                \"Rank\": rank + 1,\n",
        "                \"Character\": unicode_char,\n",
        "                \"CodePoint\": f\"U+{code_point:04X}\",\n",
        "                \"Similarity_Distance\": float(distance) # 거리가 낮을수록 유사\n",
        "            })\n",
        "            \n",
        "    return results\n",
        "\n",
        "# --- 사용 예시 ---\n",
        "input_char = \"한\"\n",
        "similar_chars = find_similar_unicode(input_char, k=10)\n",
        "\n",
        "print(f\"--- '{input_char}'와 시각적으로 유사한 상위 10개 유니코드 문자 ---\")\n",
        "for result in similar_chars:\n",
        "    print(\n",
        "        f\"[{result['Rank']}] {result['Character']} \"\n",
        "        f\"({result['CodePoint']}) - Distance: {result['Similarity_Distance']:.4f}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNIVlZKAUpq3Eje6DwJB69U",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
