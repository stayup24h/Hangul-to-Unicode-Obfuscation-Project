{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIVlZKAUpq3Eje6DwJB69U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stayup24h/Hangul-to-Unicode-Obfuscation-Project/blob/main/unicode_similarity_search_by_OCR_vector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.model_selection import KFold\n",
        "from copy import deepcopy # 각 폴드마다 새로운 모델을 시작하기 위함"
      ],
      "metadata": {
        "id": "FeIYCo_PWuXB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S1plIz5QWj1a"
      },
      "outputs": [],
      "source": [
        "# autoencoder 정의\n",
        "\n",
        "def compiled_autoencoder(input_dim=68, latent_dim=16):\n",
        "    \"\"\"Autoencoder 모델을 정의하고 컴파일하여 반환합니다.\"\"\"\n",
        "\n",
        "    # 1. Encoder 정의\n",
        "    input_layer = tf.keras.layers.Input(shape=(input_dim,), name='input_vector')\n",
        "    x = tf.keras.layers.Dense(32, activation='relu')(input_layer)\n",
        "    latent_vector = tf.keras.layers.Dense(latent_dim, activation='relu', name='latent_vector')(x)\n",
        "    encoder = Model(input_layer, latent_vector, name='encoder')\n",
        "\n",
        "    # 2. Decoder 정의\n",
        "    latent_input = tf.keras.layers.Input(shape=(latent_dim,), name='latent_input')\n",
        "    y = tf.keras.layers.Dense(32, activation='relu')(latent_input)\n",
        "    reconstruction = tf.keras.layers.Dense(input_dim, activation='sigmoid', name='reconstruction')(y)\n",
        "    decoder = Model(latent_input, reconstruction, name='decoder')\n",
        "\n",
        "    # 3. Autoencoder 정의\n",
        "    autoencoder_output = decoder(encoder(input_layer))\n",
        "    autoencoder = Model(input_layer, autoencoder_output, name='autoencoder')\n",
        "\n",
        "    # 4. 컴파일\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder, encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# autoencoder 학습\n",
        "\n",
        "# TODO : 유니코드를 OCR한 68차원 numpy 배열 불러오기\n",
        "\n",
        "N = 200000 # numpy 배열 크기\n",
        "INPUT_DIM = 68\n",
        "X_features_all = np.random.rand(N, INPUT_DIM).astype('float32')\n",
        "LATENT_DIM = 16\n",
        "\n",
        "K_FOLDS = 10\n",
        "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "fold_results = []\n",
        "trained_encoders = []\n",
        "best_loss = float('inf')\n",
        "best_encoder = None\n",
        "\n",
        "print(f\"Starting {K_FOLDS}-Fold Cross-Validation\")\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(X_features_all)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{K_FOLDS} ---\")\n",
        "\n",
        "    X_train, X_val = X_features_all[train_index], X_features_all[val_index]\n",
        "\n",
        "    autoencoder, current_encoder = compiled_autoencoder(INPUT_DIM, LATENT_DIM)\n",
        "\n",
        "    history = autoencoder.fit(\n",
        "        x=X_train,\n",
        "        y=X_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val, X_val),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss = history.history['val_loss'][-1]\n",
        "    fold_results.append(val_loss)\n",
        "    print(f\"Fold {fold+1} Validation Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_encoder = deepcopy(current_encoder)\n",
        "\n",
        "mean_val_loss = np.mean(fold_results)\n",
        "std_val_loss = np.std(fold_results)\n",
        "\n",
        "print(\"\\n--- K-Fold Summary ---\")\n",
        "print(f\"Individual Fold Losses: {fold_results}\")\n",
        "print(f\"Mean Validation Loss across {K_FOLDS} Folds: {mean_val_loss:.6f} (+/- {std_val_loss:.6f})\")\n",
        "\n",
        "if best_encoder:\n",
        "    best_encoder.save('best_kf_encoder_for_faiss.h5')\n",
        "    print(\"\\nBest Encoder Model saved successfully!\")"
      ],
      "metadata": {
        "id": "lLGfyvR_c3J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 68차원 벡터들을 16차원 벡터들로 변환\n",
        "\n",
        "best_encoder = load_model('best_kf_encoder_for_faiss.h5')\n",
        "\n",
        "# TODO : 유니코드를 OCR한 68차원 numpy 배열 불러오기\n",
        "\n",
        "# X_features_all = np.load('all_unicode_features.npy')\n",
        "N = 200000 #numpy 배열 크기\n",
        "INPUT_DIM = 68\n",
        "X_features_all = np.random.rand(N, INPUT_DIM).astype('float32') # 예시 데이터\n",
        "\n",
        "X_16dim_vectors = best_encoder.predict(X_features_all)\n",
        "\n",
        "X_16dim_vectors = X_16dim_vectors.astype('float32')"
      ],
      "metadata": {
        "id": "G02TOIlKeV9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 변환한 벡터들 저장\n",
        "\n",
        "VECTOR_OUTPUT_FILENAME = \"faiss_16dim_vectors.npy\"\n",
        "\n",
        "np.save(VECTOR_OUTPUT_FILENAME, X_16dim_vectors)\n",
        "\n",
        "print(f\"16차원 잠재 벡터가 {VECTOR_OUTPUT_FILENAME}에 저장되었습니다. (Shape: {X_16dim_vectors.shape})\")"
      ],
      "metadata": {
        "id": "RWRgt5XDe0nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유니코드 코드 포인트 배열 생성 (N개의 요소)\n",
        "# 이 배열은 X_features_all을 생성했을 때의 순서와 정확히 일치해야 합니다.\n",
        "# 예시: 한글 외 유니코드 코드 포인트만 순서대로 나열\n",
        "# [0x0000, 0x0001, ..., 0x00A0, ..., 0x10FFFF] (제외 범위 제외)\n",
        "unicode_codepoints = np.array([\n",
        "    # 실제 데이터셋을 만들 때 사용된 코드 포인트를 순서대로 여기에 배치해야 함.\n",
        "    i for i in range(N) # 예시에서는 N개의 임의 인덱스라고 가정\n",
        "])\n",
        "\n",
        "MAPPING_OUTPUT_FILENAME = \"faiss_unicode_map.npy\"\n",
        "np.save(MAPPING_OUTPUT_FILENAME, unicode_codepoints)\n",
        "\n",
        "print(f\"유니코드 코드 포인트 매핑이 {MAPPING_OUTPUT_FILENAME}에 저장되었습니다. (Shape: {unicode_codepoints.shape})\")"
      ],
      "metadata": {
        "id": "YIbA6EVsfGgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}